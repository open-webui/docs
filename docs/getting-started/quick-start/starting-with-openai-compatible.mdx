---

sidebar_position: 5
title: "Starting with OpenAI-Compatible Servers"

---

## Overview

Open WebUI isn't just for OpenAI/Ollama/Llama.cpp‚Äîyou can connect **any server that implements the OpenAI-compatible API**, running locally or remotely. This is perfect if you want to run different language models, or if you already have a favorite backend or ecosystem.

---

## Protocol-Oriented Design

Open WebUI is built around **Standard Protocols**. Instead of building specific modules for every individual AI provider (like Anthropic, Gemini, or Mistral), Open WebUI supports the **OpenAI Chat Completions Protocol**. 

Any provider that offers an OpenAI-compatible endpoint can be used with Open WebUI. This approach ensure maximum compatibility with minimal configuration bloat. 

### Popular Compatible Servers and Providers

There are many servers and tools that expose an OpenAI-compatible API. Pick whichever suits your workflow:

- **Local Runners**: [Llama.cpp](https://github.com/ggml-org/llama.cpp), [Ollama](https://ollama.com/), [LM Studio](https://lmstudio.ai/), [LocalAI](https://localai.io/), [Docker Model Runner](https://docs.docker.com/ai/model-runner/), [Lemonade](https://lemonade-server.ai/).
- **Cloud Providers**: [Groq](https://groq.com/), [Mistral AI](https://mistral.ai/), [Perplexity](https://www.perplexity.ai/), [MiniMax](https://platform.minimax.io/), [OpenRouter](https://openrouter.ai/), [LiteLLM](https://docs.litellm.ai/).
- **Google Gemini**: Google also provides an OpenAI-compatible endpoint for Gemini models.
  - **Endpoint**: `https://generativelanguage.googleapis.com/v1beta/openai/`
  - **Key**: Use your Gemini API key from [Google AI Studio](https://aistudio.google.com/) or Vertex AI.

---

## Step 1: Connect Your Server to Open WebUI

#### üçã Get Started with Lemonade

Lemonade is a plug-and-play ONNX-based OpenAI-compatible server. Here‚Äôs how to try it on Windows:

1. [Download the latest `.exe`](https://github.com/lemonade-sdk/lemonade)
2. Run `Lemonade_Server_Installer.exe`
3. Install and download a model using Lemonade‚Äôs installer
4. Once running, your API endpoint will be:

   ```
   http://localhost:8000/api/v0
   ```

![Lemonade Server](/images/getting-started/lemonade-server.png)

See [their docs](https://lemonade-server.ai/) for details.

---

## Step 2: Connect Your Server to Open WebUI

1. Open Open WebUI in your browser.
2. Go to ‚öôÔ∏è **Admin Settings** ‚Üí **Connections** ‚Üí **OpenAI**.
3. Click ‚ûï **Add Connection**.
4. Select the **Standard / Compatible** tab (if available).
5. Fill in the following:
   - **API URL**: Use your server‚Äôs API endpoint.
     *   **Examples**: `http://localhost:11434/v1` (Ollama), `http://localhost:10000/v1` (Llama.cpp).
   - **API Key**: Leave blank unless the server requires one.
6. Click **Save**.

:::tip Connection Timeout Configuration

If your local server is slow to start or you're connecting over a high-latency network, you can adjust the model list fetch timeout:

```bash
# Adjust timeout for slower connections (default is 10 seconds)
AIOHTTP_CLIENT_TIMEOUT_MODEL_LIST=5
```

If you've saved an unreachable URL and the UI becomes unresponsive, see the [Model List Loading Issues](/troubleshooting/connection-error#Ô∏è-model-list-loading-issues-slow-ui--unreachable-endpoints) troubleshooting guide for recovery options.

:::

:::tip

If running Open WebUI in Docker and your model server on your host machine, use `http://host.docker.internal:<your-port>/v1`.

:::

##### **For Lemonade:**  When adding Lemonade, use `http://localhost:8000/api/v0` as the URL.

![Lemonade Connection](/images/getting-started/lemonade-connection.png)

---

## Required API Endpoints

To ensure full compatibility with Open WebUI, your server should implement the following OpenAI-standard endpoints:

| Endpoint | Method | Required? | Purpose |
| :--- | :--- | :--- | :--- |
| `/v1/models` | `GET` | **Yes** | Used for model discovery and selecting models in the UI. |
| `/v1/chat/completions` | `POST` | **Yes** | The core endpoint for chat, supporting streaming and parameters like temperature. |
| `/v1/embeddings` | `POST` | No | Required if you want to use this provider for RAG (Retrieval Augmented Generation). |
| `/v1/audio/speech` | `POST` | No | Required for Text-to-Speech (TTS) functionality. |
| `/v1/audio/transcriptions` | `POST` | No | Required for Speech-to-Text (STT/Whisper) functionality. |
| `/v1/images/generations` | `POST` | No | Required for Image Generation (DALL-E) functionality. |

### Supported Parameters

Open WebUI passes standard OpenAI parameters such as `temperature`, `top_p`, `max_tokens` (or `max_completion_tokens`), `stop`, `seed`, and `logit_bias`. It also supports **Tool Use** (Function Calling) if your model and server support the `tools` and `tool_choice` parameters.

---

## Step 3: Start Chatting!

Select your connected server‚Äôs model in the chat menu and get started!

That‚Äôs it! Whether you choose Llama.cpp, Ollama, LM Studio, or Lemonade, you can easily experiment and manage multiple model servers‚Äîall in Open WebUI.

---

üöÄ Enjoy building your perfect local AI setup!