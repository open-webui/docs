---
sidebar_position: 2
title: "OpenAI"

---

## Overview

Open WebUI makes it easy to connect and use OpenAI and other OpenAI-compatible APIs. This guide will walk you through adding your API key, setting the correct endpoint, and selecting models ‚Äî so you can start chatting right away.

---

## Important: Protocols, Not Providers

Open WebUI is a **protocol-centric** platform. While we provide first-class support for OpenAI models, we do so mainly through the **OpenAI Chat Completions API protocol**. 

We focus on universal standards shared across dozens of providers, with experimental support for emerging standards like **[Open Responses](https://www.openresponses.org/)**. For a detailed explanation, see our **[FAQ on protocol support](/faq#q-why-doesnt-open-webui-natively-support-provider-xs-proprietary-api)**.

---

## Step 1: Get Your OpenAI API Key

To use OpenAI models (such as GPT-4 or o3-mini), you need an API key from a supported provider.

You can use:

- **OpenAI** directly (https://platform.openai.com/account/api-keys)
- **Azure OpenAI**
- **Anthropic** (via their [OpenAI-compatible endpoint](https://platform.claude.com/docs/en/api/openai-sdk))
- **Google Gemini** (via their [OpenAI-compatible endpoint](https://generativelanguage.googleapis.com/v1beta/openai/))
- **DeepSeek** (https://platform.deepseek.com/)
- **MiniMax** (https://platform.minimax.io/)
- **Proxies & Aggregators**: OpenRouter, LiteLLM, Helicone, Vercel AI Gateway.
- **Local Servers**: Ollama, Llama.cpp, LM Studio, vLLM, LocalAI.

---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## Step 2: Add the API Connection in Open WebUI

Once Open WebUI is running:

1. Go to the ‚öôÔ∏è **Admin Settings**.
2. Navigate to **Connections > OpenAI > Manage** (look for the wrench icon).
3. Click ‚ûï **Add New Connection**.

<Tabs>
  <TabItem value="standard" label="Standard / Compatible" default>

  Use this for **OpenAI**, **DeepSeek**, **MiniMax**, **OpenRouter**, **LocalAI**, **FastChat**, **Helicone**, **LiteLLM**, **Vercel AI Gateway** etc.

  *   **Connection Type**: External
  *   **URL**: `https://api.openai.com/v1` (or your provider's endpoint)
  *   **API Key**: Your secret key (usually starts with `sk-...`)

  </TabItem>
  <TabItem value="azure" label="Azure OpenAI">

  For Microsoft Azure OpenAI deployments.

  1.  Find **Provider Type** and click the button labeled **OpenAI** to switch it to **Azure OpenAI**.
  2.  **URL**: Your Azure Endpoint (e.g., `https://my-resource.openai.azure.com`).
  3.  **API Version**: e.g., `2024-02-15-preview`.
  4.  **API Key**: Your Azure API Key.
  5.  **Model IDs (Deployments)**: You **must** add your specific Deployment Names here (e.g., `my-gpt4-deployment`).

  </TabItem>
</Tabs>

### Advanced Configuration

*   **Model IDs (Filter)**:
    *   *Default (Empty)*: Auto-detects all available models from the provider.
    *   *Set*: Acts as an **Allowlist**. Only the specific model IDs you enter here will be visible to users. Use this to hide older or expensive models.
    
    :::tip OpenRouter Recommendation
    When using **OpenRouter**, we **highly recommend**:
    1.  **Use an allowlist** (add specific Model IDs). OpenRouter exposes thousands of models, which can clutter your model selector and slow down the admin panel if not filtered.
    2.  **Enable Model Caching** (`Settings > Connections > Cache Base Model List` or `ENABLE_BASE_MODELS_CACHE=True`). Without caching, page loads can take 10-15+ seconds on first visit due to querying a large number of models. See the [Performance Guide](/troubleshooting/performance) for more details.
    :::

    :::caution MiniMax Whitelisting
    Some providers, like **MiniMax**, do not expose their models via a `/models` endpoint. For these providers, you **must** manually add the Model ID (e.g., `MiniMax-M2.5`) to the **Model IDs (Filter)** list for them to appear in the UI.
    :::
    
*   **Prefix ID**:
    *   If you connect multiple providers that have models with the same name (e.g., two providers both offering `llama3`), add a prefix here (e.g., `groq/`) to distinguish them. The model will appear as `groq/llama3`.

4. Click **Save** ‚úÖ.

This securely stores your credentials.

:::tip Connection Timeout Configuration

If your API provider is slow to respond or you're experiencing timeout issues, you can adjust the model list fetch timeout:

```bash
# Increase timeout for slow networks (default is 10 seconds)
AIOHTTP_CLIENT_TIMEOUT_MODEL_LIST=15
```

If you've saved an unreachable URL and the UI becomes unresponsive, see the [Model List Loading Issues](/troubleshooting/connection-error#Ô∏è-model-list-loading-issues-slow-ui--unreachable-endpoints) troubleshooting guide for recovery options.

:::

![OpenAI Connection Screen](/images/getting-started/quick-start/manage-openai.png)

---

## Step 3: Start Using Models

Once your connection is saved, you can start using models right inside Open WebUI.

üß† You don‚Äôt need to download any models ‚Äî just select one from the Model Selector and start chatting. If a model is supported by your provider, you‚Äôll be able to use it instantly via their API.

Here‚Äôs what model selection looks like:

![OpenAI Model Selector](/images/getting-started/quick-start/selector-openai.png)

Simply choose GPT-4, o3-mini, or any compatible model offered by your provider.

---

## All Set!

That‚Äôs it! Your OpenAI-compatible API connection is ready to use.

With Open WebUI and OpenAI, you get powerful language models, an intuitive interface, and instant access to chat capabilities ‚Äî no setup headaches.

If you run into issues or need additional support, visit our [help section](/troubleshooting).

Happy prompting! üéâ
