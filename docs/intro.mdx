---
sidebar_position: 0
slug: /
title: üè° Home
hide_title: true
---

import { TopBanners } from "@site/src/components/TopBanners";
import { SponsorList } from "@site/src/components/SponsorList";

# Open WebUI

<TopBanners />

**Open WebUI is an [extensible](https://github.com/open-webui/pipelines), feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline.** It supports various LLM runners, including Ollama and OpenAI-compatible APIs.

![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Follama-webui%2Follama-wbui&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)

![Open WebUI Demo](/img/demo.gif)

:::info **Important Note on User Roles and Privacy:**

- **Admin Creation:** The first account created on Open WebUI gains **Administrator privileges**, controlling user management and system settings.

- **User Registrations:** Subsequent sign-ups start with **Pending** status, requiring Administrator approval for access.

- **Privacy and Data Security:** **All your data**, including login details, is **locally stored** on your device. Open WebUI ensures **strict confidentiality** and **no external requests** for enhanced privacy and security.

:::

:::tip

#### Disabling Login for Single User

If you want to disable login for a single-user setup, set [`WEBUI_AUTH`](/getting-started/env-configuration) to `False`. This will bypass the login page.

:::warning
You cannot switch between single-user mode and multi-account mode after this change.
:::

## Quick Start with Docker üê≥ (Recommended)

:::danger
When using Docker to install Open WebUI, make sure to include the `-v open-webui:/app/backend/data` in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.
:::

<details> 
<summary>Data Storage in Docker</summary>

This tutorial uses [Docker named volumes](https://docs.docker.com/storage/volumes/) to guarantee the **persistance of your data**. This might make it difficult to know exactly where your data is stored in your machine if this is your first time using Docker. Alternatively, you can replace the volume name with a absolute path on your host machine to link your container data to a folder in your computer using a [bind mount](https://docs.docker.com/storage/bind-mounts/).

**Example**: change `-v open-webui:/app/backend/data` to `-v /path/to/folder:/app/backend/data`

Ensure you have the proper access rights to the folder on your host machine.

Visit the [Docker documentation](https://docs.docker.com/storage/) to understand more about volumes and bind mounts.
</details>

### Installation Using Docker Run

- **If Ollama is already installed on your computer**, use this command:

  ```bash
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **If Ollama is installed on a different server**, use this command:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=http://localhost:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```
  To connect to Ollama on another server, change the `OLLAMA_BASE_URL` to the server's URL.

- **To run Open WebUI with Nvidia GPU support**, use this command:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Installation for OpenAI API Usage Only

- **If you're only using OpenAI API**, use this command:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

## Installing Open WebUI With Bundled Ollama Support

This installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:

- **Bundled Ollama Installation with Nvidia GPU Support**:
  Utilize GPU resources by running the following command:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **Bundled Ollama Installation With CPU Support Only**:
  If you're not using a GPU, use this command instead:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Both commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.

## Installation Using Docker Compose

Alternatively, you can use Docker Compose to manage your Open WebUI containers. Here's an example `docker-compose.yml` file for each of the above scenarios:

- **If Ollama is already installed on your computer:**

```yml
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - WEBUI_AUTH=True
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

volumes:
  open-webui:
```

- **If Ollama is installed on a different server:**

```yml
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://localhost:11434
      - WEBUI_AUTH=True
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

volumes:
  open-webui:
```

- **Install With Nvidia GPU Support:**

```yml
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:cuda
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "3000:8080"
    deploy:
        resources:
            reservations:
                devices:
                    - driver: nvidia
                      count: all
                      capabilities:
                          - gpu
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

volumes:
  open-webui:
```

- **Install For OpenAI API Usage Only:**

```yml
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OPENAI_API_KEY=your_secret_key
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

volumes:
  open-webui:
```

- **Install For Bundled Ollama + Nvidia GPU Support:**

```yml
services:
    open-webui:
        ports:
            - 3000:8080
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - ollama:/root/.ollama
            - open-webui:/app/backend/data
        container_name: open-webui
        restart: always
        image: ghcr.io/open-webui/open-webui:ollama
volumes:
    ollama:
    open-webui:
```


- **Install For Bundled Ollama + CPU Support Only:**

```yml
services:
    open-webui:
        ports:
            - 3000:8080
        volumes:
            - ollama:/root/.ollama
            - open-webui:/app/backend/data
        container_name: open-webui
        restart: always
        image: ghcr.io/open-webui/open-webui:ollama
volumes:
    ollama:
    open-webui:
```


### Starting the Containers

Once you've created the `docker-compose.yml` file, you can start the containers using the following command. However, to ensure that you're using the latest images, run `docker-compose pull` first:

```bash
docker-compose pull
docker-compose up -d
```

This will download any updates to the images and then start the containers in detached mode. You can then access Open WebUI at [http://localhost:3000](http://localhost:3000).

## Using the Dev Branch üåô

:::warning
The `:dev` branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features. It is not recommended for production environments.
:::

If you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the `:dev` tag. Here's how to do it:

#### Using Docker

You can run the `:dev` branch using the following Docker command:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:dev
```

This command will start a new container named `open-webui` in detached mode, map port `3000` on the host machine to port `8080` in the container, and mount a volume named `open-webui` to persist your data.

#### Using Docker Compose

Alternatively, you can use Docker Compose to manage your container. Create a `docker-compose.yml` file with the following contents:

```yml
version: '3'

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:dev
    container_name: open-webui
    environment:
      - WEBUI_AUTH=True
      - OLLAMA_BASE_URL=https://example.com
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "3000:8080"
    restart: always

volumes:
  open-webui:
```

Then, navigate to the directory where the `docker-compose.yml` file resides and run the following command to start the container in detached mode:

```bash
docker-compose up -d
```

This will start the container and make Open WebUI available at [http://localhost:3000](http://localhost:3000).

#### Important Notes

* Make sure to regularly update the container to get the latest changes from the `:dev` branch.
* If you encounter any issues or bugs, please report them to our [https://github.com/open-webui/open-webui/issues](issue tracker) so we can address them promptly.

## Updating

Check out our full [updating guide](/getting-started/updating).

To keep your container up-to-date, you can use [https://github.com/containrrr/watchtower](Watchtower), a tool that automatically updates running Docker containers. Create a `docker-compose.yml` file with the following contents:

```yml
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - WEBUI_AUTH=True
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    restart: always

volumes:
  open-webui:
```

You can also update your container manually by running the following commands in the directory of your docker-compose.yml file:

```bash
docker-compose pull
docker compose down
docker-compose up -d
```

This will update the `open-webui` container to the latest version.

To update your container manually using Docker Run, you can use the following command:

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

Replace `open-webui` with your specified container name if it is different.

## Manual Installation

### Installation with `pip` (Beta)

For users who prefer to use Python's package manager `pip`, Open WebUI offers a installation method. Python 3.11 is required for this method.

1. **Install Open WebUI**:
   Open your terminal and run the following command:

   ```bash
   pip install open-webui
   ```

2. **Start Open WebUI**:
   Once installed, start the server using:

   ```bash
   open-webui serve
   ```

This method installs all necessary dependencies and starts Open WebUI, allowing for a simple and efficient setup. After installation, you can access Open WebUI at [http://localhost:8080](http://localhost:8080). Enjoy! üòÑ

## Other Installation Methods

We offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/) or join our [Discord community](https://discord.gg/5rJgQTnV4s) for comprehensive guidance.

## Troubleshooting

If you're facing various issues like "Open WebUI: Server Connection Error", see [TROUBLESHOOTING](troubleshooting) for information on how to troubleshoot and/or join our [Open WebUI Discord community](https://discord.gg/5rJgQTnV4s).

## Sponsors üôå

We are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!

<SponsorList />
