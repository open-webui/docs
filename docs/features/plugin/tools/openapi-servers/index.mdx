---
sidebar_position: 400
title: "OpenAPI Tool Servers"
---

import { TopBanners } from "@site/src/components/TopBanners";

<TopBanners />

# üåü OpenAPI Tool Servers

This repository provides reference OpenAPI Tool Server implementations making it easy and secure for developers to integrate external tooling and data sources into LLM agents and workflows. Designed for maximum ease of use and minimal learning curve, these implementations utilize the widely adopted and battle-tested [OpenAPI specification](https://www.openapis.org/) as the standard protocol.

By leveraging OpenAPI, we eliminate the need for a proprietary or unfamiliar communication protocol, ensuring you can quickly and confidently build or integrate servers. This means less time spent figuring out custom interfaces and more time building powerful tools that enhance your AI applications.

## ‚òùÔ∏è Why OpenAPI?

- **Established Standard**: OpenAPI is a widely used, production-proven API standard backed by thousands of tools, companies, and communities.

- **No Reinventing the Wheel**: No additional documentation or proprietary spec confusion. If you build REST APIs or use OpenAPI today, you're already set.

- **Easy Integration & Hosting**: Deploy your tool servers externally or locally without vendor lock-in or complex configurations.

- **Strong Security Focus**: Built around HTTP/REST APIs, OpenAPI inherently supports widely used, secure communication methods including HTTPS and well-proven authentication standards (OAuth, JWT, API Keys).

- **Future-Friendly & Stable**: Unlike less mature or experimental protocols, OpenAPI promises reliability, stability, and long-term community support.

## üöÄ Quickstart

Get started quickly with our reference FastAPI-based implementations provided in the `servers/` directory. (You can adapt these examples into your preferred stack as needed, such as using [FastAPI](https://fastapi.tiangolo.com/), [FastOpenAPI](https://github.com/mr-fatalyst/fastopenapi) or any other OpenAPI-compatible library):

```bash
git clone https://github.com/open-webui/openapi-servers
cd openapi-servers
```

### With Bash

```bash

# Example: Installing dependencies for a specific server 'filesystem'
cd servers/filesystem
pip install -r requirements.txt
uvicorn main:app --host 0.0.0.0 --reload
```

The filesystem server should be reachable from: [http://localhost:8000](http://localhost:8000)

The documentation path will be: [http://localhost:8000](http://localhost:8000)

### With Docker

If you have docker compose installed, bring the servers up with:

```bash
docker compose up
```

The services will be reachable from:

 * [Filesystem localhost:8081](http://localhost:8081)
 * [memory server localhost:8082](http://localhost:8082)
 * [time-server localhost:8083](http://localhost:8083)

Now, simply point your OpenAPI-compatible clients or AI agents to your local or publicly deployed URL‚Äîno configuration headaches, no complicated transports.

## üå± Open WebUI Community

- For general discussions, technical exchange, and announcements, visit our [Community Discussions](https://github.com/open-webui/openapi-servers/discussions) page.
- Have ideas or feedback? Please open an issue!
